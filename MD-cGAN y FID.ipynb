{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c70b860",
   "metadata": {},
   "source": [
    "# MD-cGAN y FID"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54225560",
   "metadata": {},
   "source": [
    "Para poder ejecutar el algoritmo en ***Kaggle*** y poder hacer uso de GPU de Kaggle insertamos el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa4891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b77022",
   "metadata": {},
   "source": [
    "## Implementación de la red MD-cGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc56d4",
   "metadata": {},
   "source": [
    "### Paquetes necesarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58fddcf",
   "metadata": {},
   "source": [
    "Importamos los paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ae00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos paquetes necesarios\n",
    "from tensorflow import keras\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Average\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58eb0d7",
   "metadata": {},
   "source": [
    "### Modelo discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el modelo discriminador\n",
    "def define_discriminator(in_shape=(28,28,1), n_classes=10):\n",
    "    # etiquetas de entrada\n",
    "    in_label = Input(shape=(1,))\n",
    "    # incrustamos las etiquetas\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    # escalamos al tamaño de las imágenes\n",
    "    n_nodes = in_shape[0] * in_shape[1]\n",
    "    li = Dense(n_nodes)(li)\n",
    "    # remodelamos un canal adicional\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    # entrada de imágenes\n",
    "    in_image = Input(shape=in_shape)\n",
    "    # concatenamos imagen y etiqueta\n",
    "    merge = Concatenate()([in_image, li])\n",
    "    # reducción por convolución 28x28 -> 14x14\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # reducción por convolución 14x14 -> 7x7\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # aplanado\n",
    "    fe = Flatten()(fe)\n",
    "    # dropout\n",
    "    fe = Dropout(0.4)(fe)\n",
    "    # output\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    # definimos el modelo\n",
    "    model = Model([in_image, in_label], out_layer)\n",
    "    # compilamos el modelo\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f5f56",
   "metadata": {},
   "source": [
    "### Modelo generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c82149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el modelo generador\n",
    "# proporcionamos el espacio latente y el número de clases\n",
    "def define_generator(latent_dim, n_classes=10):\n",
    "    # etiquetas de entrada\n",
    "    in_label = Input(shape=(1,))\n",
    "    # incrustamos las etiquetas\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    # multiplicación\n",
    "    n_nodes = 7 * 7\n",
    "    li = Dense(n_nodes)(li)\n",
    "    # remodelamos un canal adicional\n",
    "    li = Reshape((7, 7, 1))(li)\n",
    "    # entrada de imágenes\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    # base de la imagen 7x7\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((7, 7, 128))(gen)\n",
    "    # concatenamos imagen y etiqueta\n",
    "    merge = Concatenate()([gen, li])\n",
    "    # aumentamos por convolución 7x7 -> 14x14\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    # aumentamos por convolución 14x14 -> 28x28\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    # output\n",
    "    out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
    "    # definimos el modelo (no lo compilamos)\n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae306b91",
   "metadata": {},
   "source": [
    "### Modelo GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el modelo combinado de generador-discriminador, para actualizar el generador\n",
    "def define_gan(g_model, d_model, num_d):\n",
    "    y = []\n",
    "    # congelamos los modelos discriminadores para que no entrenen\n",
    "    for n_d in range(num_d):\n",
    "        d_model[n_d].trainable = False\n",
    "    # obtenemos la entrada de ruido y etiquetas del modelo generador\n",
    "    gen_noise, gen_label = g_model.input\n",
    "    # obtenemos la imagen de salida del modelo generador\n",
    "    gen_output = g_model.output\n",
    "    # conectamos la imagen y la etiqueta del generador como entradas del discriminador\n",
    "    #gan_output = d_model([gen_output, gen_label])\n",
    "    if num_d==1:\n",
    "        gan_output = d_model[0]([gen_output, gen_label])\n",
    "    else:\n",
    "        for n_d in range(num_d):\n",
    "            y.append(d_model[n_d]([gen_output, gen_label]))\n",
    "        # definimos el cálculo (Average, Maximum ó Minimum)\n",
    "        gan_output = Maximum()(y)\n",
    "    # definimos el modelo combinado gan\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    # compilamos el modelo\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d4ad4b",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos las imágenes del fashion-MNIST\n",
    "def load_real_samples():\n",
    "    # cargamos los datos de entrenamiento\n",
    "    (trainX, trainy), (_, _) = load_data()\n",
    "    # añadimos un canal -> 3D\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # convertimos de entero a flotante\n",
    "    X = X.astype('float32')\n",
    "    # escalamos [0, 255] -> [-1, 1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return [X, trainy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048724b5",
   "metadata": {},
   "source": [
    "### Selección de imágenes reales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a043eaa",
   "metadata": {},
   "source": [
    "Realizamos una selección aleatoria de imágenes reales del dataset importado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionamos aleatoriamente un grupo de imágenes reales\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # dividimos en imágenes y etiquetas\n",
    "    images, labels = dataset\n",
    "    # selección aleatoria de índices\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # selección aleatoria de imágenes y etiquetas\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generamos las etiquetas de clase 1 -> real (0 -> fake)\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c1842",
   "metadata": {},
   "source": [
    "### Espacio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e13db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos puntos en el espacio latente como entrada para el generador\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
    "    # generamos aleatoriamente puntos\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # redimensionamos\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # generamos las etiquetas\n",
    "    labels = randint(0, n_classes, n_samples)\n",
    "    return [z_input, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2510d29",
   "metadata": {},
   "source": [
    "### Generamos imágenes falsas (fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos el generador para crear imágenes falsas\n",
    "# con etiquetas de clase (0 -> fake)\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generamos puntos en el espacio latente\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predicción\n",
    "    images = g_model.predict([z_input, labels_input])\n",
    "    # generamos las etiquetas de clase 0 -> fake (1 -> real)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1201f7",
   "metadata": {},
   "source": [
    "### Guardamos grupo de imágenes (10x10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d518838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos un plot con las imágenes generadas (10x10)\n",
    "def save_plot(examples, epoch, n=10):\n",
    "    # creamos el plot\n",
    "    for i in range(n * n):\n",
    "        # subplot\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot\n",
    "        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "    # guardamos el fichero de imágenes\n",
    "    filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "    pyplot.savefig(filename)\n",
    "pyplot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3dd5c",
   "metadata": {},
   "source": [
    "### Evaluamos el modelo discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cfd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluamos el modelo discriminador, generamos imágenes y guardamos el modelo generador\n",
    "def summarize_performance(epoch, g_model, d_model, num_d, dataset, latent_dim, n_samples=100):\n",
    "    acc_real = []\n",
    "    acc_fake = []\n",
    "    #tot_acc_real = []\n",
    "    #tot_acc_fake = []\n",
    "    #tot_epoch = []\n",
    "    # muestras de imágenes reales\n",
    "    [X_real, labels_real], y_real = generate_real_samples(dataset, n_samples)\n",
    "    # muestras de imágenes fake\n",
    "    [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluamos el discriminador con las muestras reales y falsas\n",
    "    for n_d in range(num_d):\n",
    "        _, acc_real.append(d_model[n_d].evaluate([X_real, labels_real], y_real, verbose=0))\n",
    "        _, acc_fake.append(d_model[n_d].evaluate([X_fake, labels_fake], y_fake, verbose=0))\n",
    "        # rendimiento del discriminador\n",
    "        print('>Accuracy real_%d: %.0f%%, fake_%d: %.0f%%' % (n_d+1, acc_real[n_d][-1]*100, n_d+1, acc_fake[n_d][-1]*100))\n",
    "        # guardamos el rendimiento del discriminador\n",
    "        #tot_acc_real.append(acc_real[n_d]*100)\n",
    "        #tot_acc_fake.append(acc_fake[n_d]*100)\n",
    "    #tot_epoch.append(epoch +1)\n",
    "    # muestras de imágenes fake\n",
    "    latent_points, labels = generate_latent_points(100, 100)\n",
    "    # etiquetas\n",
    "    labels = asarray([x for _ in range(10) for x in range(10)])\n",
    "    # generamos las imágenes fake\n",
    "    X  = g_model.predict([latent_points, labels])\n",
    "    # escalamos [-1, 1] -> [0, 1]\n",
    "    X = (X + 1) / 2.0\n",
    "    # salvamos el gráfico\n",
    "    save_plot(X, epoch)\n",
    "    \n",
    "    # guardamos el modelo generador\n",
    "    filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce9910",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo generador y discriminador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e60fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos el modelo generador y discriminador\n",
    "def train(g_model, d_model, num_d, gan_model, dataset, latent_dim, n_epochs, n_batch):\n",
    "    d_loss_real = []\n",
    "    d_loss_fake = []\n",
    "\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # recorremos las épocas\n",
    "    for i in range(n_epochs):\n",
    "        # recorremos los lotes definidos por época\n",
    "        for j in range(bat_per_epo):\n",
    "            # muestras aleatorias de imágenes reales\n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "            # muestras de imágenes fake\n",
    "            [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # actualizamos los pesos del modelo discriminador\n",
    "            for n_d in range(num_d):\n",
    "                d_loss_real.append(d_model[n_d].train_on_batch([X_real, labels_real], y_real))\n",
    "                d_loss_fake.append(d_model[n_d].train_on_batch([X_fake, labels_fake], y_fake))\n",
    "                \n",
    "                \n",
    "            # preparamos puntos en el espacio latente como entrada para el modelo generador\n",
    "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            # creamos etiquetas falsas para las muestras fake (en lugar de 0)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # actualizamos el modelo generador mediante el error del discriminador\n",
    "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "            \n",
    "            # pérdidas del lote\n",
    "            for n_d in range(num_d):\n",
    "                print('>%d, %d/%d, d_real_%d=%.3f, d_fake%d=%.3f g=%.3f' %\n",
    "                (i+1, j+1, bat_per_epo, n_d+1, d_loss_real[n_d][-1], n_d+1, d_loss_fake[n_d][-1], g_loss))\n",
    "\n",
    "        # evaluamos el rendimiento del modelo cada 10 épocas\n",
    "        if (i+1) % 10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, num_d, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f64c1",
   "metadata": {},
   "source": [
    "### Parametros de la GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# épocas\n",
    "n_epochs = 200\n",
    "# lote\n",
    "n_batch = 128\n",
    "# tamaño del espacio latente (100, 10, 50, 500)\n",
    "latent_dim = 100\n",
    "# num discriminators\n",
    "num_d = 3\n",
    "\n",
    "# creamos los modelos discriminadores\n",
    "d_model = []\n",
    "for n_d in range(num_d):\n",
    "     d_model.append(define_discriminator())\n",
    "    \n",
    "# creamos el modelo generador\n",
    "g_model = define_generator(latent_dim)\n",
    "# creamos el modelo combinado\n",
    "gan_model = define_gan(g_model, d_model, num_d)\n",
    "# cargamos imágenes reales\n",
    "dataset = load_real_samples()\n",
    "# entrenamos la red\n",
    "train(g_model, d_model, num_d, gan_model, dataset, latent_dim, n_epochs, n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8120b",
   "metadata": {},
   "source": [
    "## Cálculo del FID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0c783",
   "metadata": {},
   "source": [
    "Una vez que tenemos los diferentes modelos obtenidos para las distintas épocas de entrenamiento tenemos que evaluar lo bien que genera imágenes ese modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71293f05",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956f593",
   "metadata": {},
   "source": [
    "Para llevar a cabo el cálculo del FID necesitamos evaluar las imágenes mediante un modelo para ello utilizamos el modelo ya optimizado *InceptionV3*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e3397",
   "metadata": {},
   "source": [
    "### Paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el FID (Frechet inception distance) para fashion_mnist\n",
    "# importamos paquetes necesarios\n",
    "import numpy\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import shuffle\n",
    "from scipy.linalg import sqrtm\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.datasets.mnist import load_data\n",
    "from skimage.transform import resize\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6878f2",
   "metadata": {},
   "source": [
    "### Escalado de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# escalamos las imágenes\n",
    "def scale_images(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        # guardamos\n",
    "        images_list.append(new_image)\n",
    "    return asarray(images_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bd167",
   "metadata": {},
   "source": [
    "### Calculamos el FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303fe936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cálculo del FID\n",
    "def calculate_fid(model, images1, images2):\n",
    "    # predicción del modelo InceptionV3\n",
    "    act1 = model.predict(images1)\n",
    "    act2 = model.predict(images2)\n",
    "    # calculamos la media y covarianza\n",
    "    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "    # calculamos la suma de los cuadrados de la difencia de medias\n",
    "    ssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
    "    # calculamos la raiz cuadrada del producto de las covarianzas\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # comprobamos y corregimos los números imaginarios de la raiz\n",
    "    if iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculamos el FID\n",
    "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec6f86",
   "metadata": {},
   "source": [
    "### Preparamos el modelo *Inception v3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepamos el modelo inception v3 (discriminador)\n",
    "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f220a",
   "metadata": {},
   "source": [
    "### Preparamos los modelos generadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos imágenes con modelo generador\n",
    "\n",
    "# importamos paquetes\n",
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.models import load_model\n",
    "\n",
    "# parametros\n",
    "n_paquetes = 100\n",
    "n_classes = 10\n",
    "num_imag = n_paquetes * n_classes\n",
    "\n",
    "# generamos espacio latente\n",
    "latent_points, labels = generate_latent_points(100, num_imag, n_classes)\n",
    "\n",
    "# etiquetas\n",
    "labels = asarray([x for _ in range(n_paquetes) for x in range(n_classes)])\n",
    "\n",
    "# cargamos el modelo\n",
    "fid_epoch = []\n",
    "epoch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef3136",
   "metadata": {},
   "source": [
    "### Generamos pool de imágenes reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b509ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos imágenes reales\n",
    "dataset = load_real_samples()\n",
    "n_samples = 1000\n",
    "[X_real, labels_real], y_real = generate_real_samples(dataset, n_samples)\n",
    "X_real = X_real.astype('float32')\n",
    "\n",
    "#escalamos las imágenes\n",
    "X_real = scale_images(X_real, (299,299,3))\n",
    "\n",
    "# preprocesamos imágenes\n",
    "X_real = preprocess_input(X_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847d4e7",
   "metadata": {},
   "source": [
    "### Calculamos FID para todos los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacc1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    model_guardado = 'generator_model_%03d.h5' % ((i + 1) * 10)\n",
    "    model_guardado = load_model(model_guardado)\n",
    "    \n",
    "    # generamos imágenes\n",
    "    X_fake = model_guardado.predict([latent_points, labels])\n",
    "    X_fake = X_fake.astype('float32')\n",
    "    # escalamos las imágenes\n",
    "    X_fake = scale_images(X_fake, (299,299,3))\n",
    "    print('Scaled', X_real.shape, X_fake.shape)\n",
    "    # preprocesamos imágenes\n",
    "    X_fake = preprocess_input(X_fake)\n",
    "    # calculamos el fid\n",
    "    fid = calculate_fid(model, X_real, X_fake)\n",
    "    fid_epoch.append(fid)\n",
    "    epoch.append((i+1) * 10)\n",
    "    print('FID: %.3f' % fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124fc5d7",
   "metadata": {},
   "source": [
    "### Gráfico del FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90252d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graficamos\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "x = epoch\n",
    "y = fid_epoch\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.clf()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.clear()\n",
    "ax.plot(x, y,  marker='D')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('fid')\n",
    "ax.set_title('FID por épocas')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75603a17",
   "metadata": {},
   "source": [
    "Podemos guardar el gráfico resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"FID.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df33a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
